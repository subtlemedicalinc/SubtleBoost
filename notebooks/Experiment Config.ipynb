{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "from collections import OrderedDict\n",
    "\n",
    "def trans(st):\n",
    "    try:\n",
    "        return int(st)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return float(st)\n",
    "        except Exception:\n",
    "            return st\n",
    "\n",
    "def intersection(dicts):\n",
    "    return dict(set.intersection(*(set(d.items()) for d in dicts)))\n",
    "\n",
    "def flatten(conf_dict):\n",
    "    new_dict = OrderedDict({})\n",
    "    new_sub = OrderedDict({})\n",
    "    for key, val in conf_dict.items():\n",
    "        if not isinstance(val, dict):\n",
    "            new_dict[key] = val\n",
    "        else:\n",
    "            new_sub[key] = val\n",
    "    return new_dict, new_sub\n",
    "\n",
    "exp_name = 'stanford_sri'\n",
    "\n",
    "train_data = 'data_train_stanford_sri.txt'\n",
    "test_data = 'data_test_stanford_sri.txt'\n",
    "\n",
    "dirpath_base = '/Users/srivathsa/projects/SubtleGad'\n",
    "\n",
    "# data_dict = {\n",
    "#     'train': open('{}/data_lists/{}'.format(dirpath_base, train_data)).read().strip().split('\\n'),\n",
    "#     'test': open('{}/data_lists/{}'.format(dirpath_base, test_data)).read().strip().split('\\n'),\n",
    "# }\n",
    "\n",
    "# with open('{}/experiments/{}/data.json'.format(dirpath_base, exp_name), 'w') as fpath_json:\n",
    "#     st = json.dumps(data_dict, indent=2)\n",
    "#     fpath_json.write(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dict = {\n",
    "    \"gpu\": 2,\n",
    "    \"data_dir\": \"/raid/jon/data_full_tiantan/data\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"max_data_sets\": 100,\n",
    "    \"batch_size\": 8,\n",
    "    \"num_epochs\": 100,\n",
    "    \"queue_size\": 4,\n",
    "    \"slices_per_input\": 7,\n",
    "    \"shuffle\": True,\n",
    "    \"file_ext\": \"h5\",\n",
    "    \"val_split\": 0.2,\n",
    "    \"verbose\": 1,\n",
    "    \"log_dir\": \"/raid/jon/logs\",\n",
    "    \"tb_dir\": \"/raid/jon/logs_tb\",\n",
    "    \"hist_dir\": \"/raid/jon/history\",\n",
    "    \"checkpoint_dir\": \"/raid/jon/checkpoints\",\n",
    "    \"checkpoint_name\": \"mres_vgg_loss\",\n",
    "    \"l1_lambda\": 0.15,\n",
    "    \"ssim_lambda\": 0.15,\n",
    "    \"no_save_best_only\": True,\n",
    "    \"multiprocessing\": 0,\n",
    "    \"num_workers\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = sorted(['train_tiantan_mpr_40data_20190626.sh', 'train_tiantan_ax_20190619.sh', 'train_tiantan_mpr_20190711.sh', 'train_tiantan_sag_20190619.sh', 'train_tiantan_mpr_20190626.sh', 'train_tiantan_mpr_2d_20190705.sh', 'train_tiantan_cor_20190619.sh', 'train_tiantan_20190612.sh'])\n",
    "\n",
    "config_dicts = []\n",
    "\n",
    "for fp in flist:\n",
    "    fpath = '../scripts/{}'.format(fp)\n",
    "    fstrs = open(fpath, 'r').read().strip().split(' ')[:-1]\n",
    "    fdict = {l.split('=')[0].lower():trans(l.split('=')[1]) for l in fstrs}\n",
    "    fdict['keystr'] = fp.replace('.sh', '').replace('train_tiantan_', '')\n",
    "    config_dicts.append(fdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data_dir\": \"/raid/jon/data_full_tiantan/data\",\n",
      "  \"20190612\": {\n",
      "    \"data_list\": \"data_lists/data_train_tiantan_20190612.txt\",\n",
      "    \"l1_lambda\": 0.6,\n",
      "    \"ssim_lambda\": 0.4,\n",
      "    \"val_split\": 0.1\n",
      "  },\n",
      "  \"batch_size\": 8,\n",
      "  \"num_epochs\": 100,\n",
      "  \"max_data_sets\": 100,\n",
      "  \"learning_rate\": 0.001,\n",
      "  \"no_save_best_only\": true,\n",
      "  \"file_ext\": \"h5\",\n",
      "  \"checkpoint_name\": \"mres_vgg_loss\",\n",
      "  \"num_workers\": 4,\n",
      "  \"gpu\": 2,\n",
      "  \"checkpoint_dir\": \"/raid/jon/checkpoints\",\n",
      "  \"slices_per_input\": 7,\n",
      "  \"log_dir\": \"/raid/jon/logs\",\n",
      "  \"verbose\": 1,\n",
      "  \"shuffle\": true,\n",
      "  \"multiprocessing\": 0,\n",
      "  \"hist_dir\": \"/raid/jon/history\",\n",
      "  \"tb_dir\": \"/raid/jon/logs_tb\",\n",
      "  \"queue_size\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cdict = config_dicts[0]\n",
    "exp = {**base_dict}\n",
    "sub_exp = {**cdict}\n",
    "\n",
    "for key, val in cdict.items():\n",
    "    if key in exp:\n",
    "        if exp[key] == val:\n",
    "            del sub_exp[key]\n",
    "        else:\n",
    "            del exp[key]\n",
    "keystr = sub_exp['keystr']\n",
    "del sub_exp['keystr']\n",
    "exp[keystr] = sub_exp\n",
    "\n",
    "print(json.dumps(exp, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/srivathsa/projects/exp_tmp/super_model/config.json\n",
      "/Users/srivathsa/projects/exp_tmp/hoag/config.json\n",
      "/Users/srivathsa/projects/exp_tmp/tiantan/config.json\n",
      "/Users/srivathsa/projects/exp_tmp/stanford_sri/config.json\n",
      "/Users/srivathsa/projects/exp_tmp/tiantan_20190711/config.json\n",
      "/Users/srivathsa/projects/exp_tmp/tiantan_sri/config.json\n",
      "/Users/srivathsa/projects/exp_tmp/tiantan_gan/config.json\n",
      "/Users/srivathsa/projects/exp_tmp/gen_siemens/config.json\n",
      "/Users/srivathsa/projects/exp_tmp/sford_siemens/config.json\n",
      "/Users/srivathsa/projects/exp_tmp/hoag_sri/config.json\n",
      "/Users/srivathsa/projects/exp_tmp/tiantan_20190626/config.json\n",
      "/Users/srivathsa/projects/exp_tmp/hoag_noscale/config.json\n"
     ]
    }
   ],
   "source": [
    "for fpath_json in glob('/Users/srivathsa/projects/exp_tmp/*/config.json'):\n",
    "    print(fpath_json)\n",
    "    config_dict = json.loads(open(fpath_json, 'r').read())\n",
    "    \n",
    "    pre, pre_sub = flatten(config_dict['preprocess'])\n",
    "    train, train_sub = flatten(config_dict['train'])\n",
    "    inf, inf_sub = flatten(config_dict['inference'])\n",
    "    common = []\n",
    "    common.append(intersection([pre, train, inf]))\n",
    "    common.append(intersection([pre, train]))\n",
    "    common.append(intersection([pre, inf]))\n",
    "    common.append(intersection([train, inf]))\n",
    "    \n",
    "    cdict = OrderedDict({k:v for d in common for k,v in d.items()})\n",
    "    cdict['gpu'] = 0\n",
    "    for key, val in cdict.items():\n",
    "        if key in pre and pre[key] == val:\n",
    "            del pre[key]\n",
    "        \n",
    "        if key in train and train[key] == val:\n",
    "            del train[key]\n",
    "        \n",
    "        if key in inf and inf[key] == val:\n",
    "            del inf[key]\n",
    "        \n",
    "        if 'gpu' in train:\n",
    "            del train['gpu']\n",
    "        if 'gpu' in inf:\n",
    "            del inf['gpu']\n",
    "        \n",
    "    cdict['preprocess'] = OrderedDict(pre)\n",
    "    for k, v in pre_sub.items():\n",
    "        cdict['preprocess'][k] = v\n",
    "        \n",
    "    cdict['train'] = OrderedDict(train)\n",
    "    for k, v in train_sub.items():\n",
    "        cdict['train'][k] = v\n",
    "    \n",
    "    cdict['inference'] = OrderedDict(inf)\n",
    "    for k, v in inf_sub.items():\n",
    "        cdict['inference'][k] = v\n",
    "    \n",
    "    json_str = json.dumps(cdict, indent=2)\n",
    "    outpath_json = fpath_json.replace('exp_tmp', 'SubtleGad/experiments')\n",
    "    with open(outpath_json, 'w') as jsonf:\n",
    "        jsonf.write(json_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py35gad)",
   "language": "python",
   "name": "py35gad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
