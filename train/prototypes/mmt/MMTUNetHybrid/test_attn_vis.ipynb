{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25535b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srivathsa/miniconda3/envs/mmt/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---final upsample expand_first---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MMT(\n",
       "  (heads): ModuleList(\n",
       "    (0): Head(\n",
       "      (model): Sequential(\n",
       "        (0): Conv2d(1, 6, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (1): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Head(\n",
       "      (model): Sequential(\n",
       "        (0): Conv2d(1, 6, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (1): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Head(\n",
       "      (model): Sequential(\n",
       "        (0): Conv2d(1, 6, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (1): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Head(\n",
       "      (model): Sequential(\n",
       "        (0): Conv2d(1, 6, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (1): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (tails): ModuleList(\n",
       "    (0): Tail(\n",
       "      (model): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): Conv2d(6, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (3): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): Tail(\n",
       "      (model): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): Conv2d(6, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (3): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): Tail(\n",
       "      (model): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): Conv2d(6, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (3): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (3): Tail(\n",
       "      (model): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): Conv2d(6, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (3): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (contrast_embeds): ModuleList(\n",
       "    (0): Embedding(4, 96)\n",
       "    (1): Embedding(4, 192)\n",
       "    (2): Embedding(4, 384)\n",
       "    (3): Embedding(4, 768)\n",
       "  )\n",
       "  (contrast_tokens): ModuleList(\n",
       "    (0): Embedding(4, 96)\n",
       "    (1): Embedding(4, 192)\n",
       "    (2): Embedding(4, 384)\n",
       "    (3): Embedding(4, 768)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0, inplace=False)\n",
       "  (encoder): SwinUNet(\n",
       "    (pos_drop): Dropout(p=0, inplace=False)\n",
       "    (layers_enc): ModuleList(\n",
       "      (0): SwinUNetEncoderLayer(\n",
       "        dim=96, input_resolution=(40, 48), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinUNetBlock(\n",
       "            dim=96, input_resolution=(40, 48), num_heads=3, window_size=[5, 6], shift_size=(0, 0), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=[5, 6], num_heads=3\n",
       "              (q): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (k): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (v): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinUNetBlock(\n",
       "            dim=96, input_resolution=(40, 48), num_heads=3, window_size=[5, 6], shift_size=(2, 3), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=[5, 6], num_heads=3\n",
       "              (q): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (k): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (v): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SwinUNetEncoderLayer(\n",
       "        dim=192, input_resolution=(20, 24), depth=2\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(40, 48), dim=96\n",
       "          (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinUNetBlock(\n",
       "            dim=192, input_resolution=(20, 24), num_heads=6, window_size=[5, 6], shift_size=(0, 0), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=[5, 6], num_heads=6\n",
       "              (q): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (k): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (v): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinUNetBlock(\n",
       "            dim=192, input_resolution=(20, 24), num_heads=6, window_size=[5, 6], shift_size=(2, 3), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=[5, 6], num_heads=6\n",
       "              (q): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (k): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (v): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SwinUNetEncoderLayer(\n",
       "        dim=384, input_resolution=(10, 12), depth=2\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(20, 24), dim=192\n",
       "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinUNetBlock(\n",
       "            dim=384, input_resolution=(10, 12), num_heads=12, window_size=[5, 6], shift_size=(0, 0), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=[5, 6], num_heads=12\n",
       "              (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinUNetBlock(\n",
       "            dim=384, input_resolution=(10, 12), num_heads=12, window_size=[5, 6], shift_size=(2, 3), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=[5, 6], num_heads=12\n",
       "              (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SwinUNetEncoderLayer(\n",
       "        dim=768, input_resolution=(5, 6), depth=2\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(10, 12), dim=384\n",
       "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinUNetBlock(\n",
       "            dim=768, input_resolution=(5, 6), num_heads=24, window_size=(5, 6), shift_size=(0, 0), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=[5, 6], num_heads=24\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinUNetBlock(\n",
       "            dim=768, input_resolution=(5, 6), num_heads=24, window_size=(5, 6), shift_size=(0, 0), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=[5, 6], num_heads=24\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers_dec): ModuleList(\n",
       "      (0): SwinUNetDecoderLayer(\n",
       "        dim=768, input_resolution=(5, 6), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinUNetBlock(\n",
       "            dim=768, input_resolution=(5, 6), num_heads=24, window_size=(5, 6), shift_size=(0, 0), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=[5, 6], num_heads=24\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinUNetBlock(\n",
       "            dim=768, input_resolution=(5, 6), num_heads=24, window_size=(5, 6), shift_size=(0, 0), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=768, window_size=[5, 6], num_heads=24\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SwinUNetDecoderLayer(\n",
       "        dim=384, input_resolution=(10, 12), depth=2\n",
       "        (upsample): PatchExpand(\n",
       "          (expand): Linear(in_features=768, out_features=1536, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (concat_linear): Linear(in_features=768, out_features=384, bias=True)\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinUNetBlock(\n",
       "            dim=384, input_resolution=(10, 12), num_heads=12, window_size=[5, 6], shift_size=(0, 0), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=[5, 6], num_heads=12\n",
       "              (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinUNetBlock(\n",
       "            dim=384, input_resolution=(10, 12), num_heads=12, window_size=[5, 6], shift_size=(2, 3), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=384, window_size=[5, 6], num_heads=12\n",
       "              (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SwinUNetDecoderLayer(\n",
       "        dim=192, input_resolution=(20, 24), depth=2\n",
       "        (upsample): PatchExpand(\n",
       "          (expand): Linear(in_features=384, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (concat_linear): Linear(in_features=384, out_features=192, bias=True)\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinUNetBlock(\n",
       "            dim=192, input_resolution=(20, 24), num_heads=6, window_size=[5, 6], shift_size=(0, 0), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=[5, 6], num_heads=6\n",
       "              (q): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (k): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (v): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinUNetBlock(\n",
       "            dim=192, input_resolution=(20, 24), num_heads=6, window_size=[5, 6], shift_size=(2, 3), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=192, window_size=[5, 6], num_heads=6\n",
       "              (q): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (k): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (v): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SwinUNetDecoderLayer(\n",
       "        dim=96, input_resolution=(40, 48), depth=2\n",
       "        (upsample): PatchExpand(\n",
       "          (expand): Linear(in_features=192, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (concat_linear): Linear(in_features=192, out_features=96, bias=True)\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinUNetBlock(\n",
       "            dim=96, input_resolution=(40, 48), num_heads=3, window_size=[5, 6], shift_size=(0, 0), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=[5, 6], num_heads=3\n",
       "              (q): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (k): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (v): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinUNetBlock(\n",
       "            dim=96, input_resolution=(40, 48), num_heads=3, window_size=[5, 6], shift_size=(2, 3), mlp_ratio=4.0\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              dim=96, window_size=[5, 6], num_heads=3\n",
       "              (q): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (k): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (v): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layers_dec): ModuleList(\n",
       "    (0): MMTDecoderLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): MMTDecoderBlock(\n",
       "          dim=768, input_resolution=(5, 6), num_heads=24, window_size=(5, 6), shift_size=(0, 0), mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): WindowAttention(\n",
       "            dim=768, window_size=(5, 6), num_heads=24\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (cross_attn): WindowAttention(\n",
       "            dim=768, window_size=(5, 6), num_heads=24\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): MMTDecoderBlock(\n",
       "          dim=768, input_resolution=(5, 6), num_heads=24, window_size=(5, 6), shift_size=(0, 0), mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): WindowAttention(\n",
       "            dim=768, window_size=(5, 6), num_heads=24\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (cross_attn): WindowAttention(\n",
       "            dim=768, window_size=(5, 6), num_heads=24\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): MMTDecoderLayer(\n",
       "      (upsample): PatchExpand(\n",
       "        (expand): Linear(in_features=768, out_features=1536, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): MMTDecoderBlock(\n",
       "          dim=384, input_resolution=(10, 12), num_heads=12, window_size=[5, 6], shift_size=(0, 0), mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): WindowAttention(\n",
       "            dim=384, window_size=[5, 6], num_heads=12\n",
       "            (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (cross_attn): WindowAttention(\n",
       "            dim=384, window_size=[5, 6], num_heads=12\n",
       "            (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): MMTDecoderBlock(\n",
       "          dim=384, input_resolution=(10, 12), num_heads=12, window_size=[5, 6], shift_size=(2, 3), mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): WindowAttention(\n",
       "            dim=384, window_size=[5, 6], num_heads=12\n",
       "            (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (cross_attn): WindowAttention(\n",
       "            dim=384, window_size=[5, 6], num_heads=12\n",
       "            (q): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (k): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (v): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): MMTDecoderLayer(\n",
       "      (upsample): PatchExpand(\n",
       "        (expand): Linear(in_features=384, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): MMTDecoderBlock(\n",
       "          dim=192, input_resolution=(20, 24), num_heads=6, window_size=[5, 6], shift_size=(0, 0), mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): WindowAttention(\n",
       "            dim=192, window_size=[5, 6], num_heads=6\n",
       "            (q): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (k): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (v): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (cross_attn): WindowAttention(\n",
       "            dim=192, window_size=[5, 6], num_heads=6\n",
       "            (q): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (k): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (v): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): MMTDecoderBlock(\n",
       "          dim=192, input_resolution=(20, 24), num_heads=6, window_size=[5, 6], shift_size=(2, 3), mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): WindowAttention(\n",
       "            dim=192, window_size=[5, 6], num_heads=6\n",
       "            (q): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (k): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (v): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (cross_attn): WindowAttention(\n",
       "            dim=192, window_size=[5, 6], num_heads=6\n",
       "            (q): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (k): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (v): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): MMTDecoderLayer(\n",
       "      (upsample): PatchExpand(\n",
       "        (expand): Linear(in_features=192, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): MMTDecoderBlock(\n",
       "          dim=96, input_resolution=(40, 48), num_heads=3, window_size=[5, 6], shift_size=(0, 0), mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): WindowAttention(\n",
       "            dim=96, window_size=[5, 6], num_heads=3\n",
       "            (q): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (k): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (v): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (cross_attn): WindowAttention(\n",
       "            dim=96, window_size=[5, 6], num_heads=3\n",
       "            (q): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (k): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (v): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): MMTDecoderBlock(\n",
       "          dim=96, input_resolution=(40, 48), num_heads=3, window_size=[5, 6], shift_size=(2, 3), mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): WindowAttention(\n",
       "            dim=96, window_size=[5, 6], num_heads=3\n",
       "            (q): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (k): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (v): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (cross_attn): WindowAttention(\n",
       "            dim=96, window_size=[5, 6], num_heads=3\n",
       "            (q): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (k): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (v): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up): FinalPatchExpand_X4(\n",
       "    (norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from networks.mmt import MMT as generator\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "from configs.config import get_config\n",
    "from datasets.dataset_brats import BRATS_dataset, RandomGeneratorBRATS\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn import Upsample\n",
    "\n",
    "split = 'test'\n",
    "batch_size = 1\n",
    "data_dir = '/mnt/raid/jiang/projects/SubtleGAN/data/brats2021_slices_crop192x160'\n",
    "ckpt_path_zerogad = '/mnt/raid/jiang/projects/SubtleGAN/MMTUNetHybrid/model/fluidstack_models/finetuned_models/MMT_single_epo100_bs32_lrg0.0005_5.0_20.0_0.0_0.1/best_mae.pth'\n",
    "ckpt_path_random = '/mnt/raid/jiang/projects/SubtleGAN/model_fluidstack/MMT/MMTUNetHybrid_GAN_ls_random_s_c_MMT_epo75_bs32_lrg0.0005_192_opt-adamw/epoch_74.pth'\n",
    "ckpt_path_single = '/mnt/raid/jiang/projects/SubtleGAN/model/MMT/MMTUNetHybrid_GAN_s_c_MMT_epo50_bs24_lrg0.0005_192_opt-adamw/epoch_49.pth'\n",
    "\n",
    "ckpt_path = ckpt_path_zerogad\n",
    "\n",
    "db = BRATS_dataset(base_dir=data_dir, split=split,\n",
    "                               transform=transforms.Compose(\n",
    "                                   [RandomGeneratorBRATS(flip=False, scale=None)]))\n",
    "dataloader = DataLoader(db, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "G = generator(img_size=[160, 192],\n",
    "                                patch_size=4,\n",
    "                                in_chans=1,\n",
    "                                out_chans = 1,\n",
    "                                embed_dim=96,\n",
    "                                depths=[2,2,2,2],\n",
    "                                num_heads=[3,6,12,24],\n",
    "                                window_size=[5,6],\n",
    "                                mlp_ratio=4.0,\n",
    "                                qkv_bias=True,\n",
    "                                qk_scale=None,\n",
    "                                drop_rate=0,\n",
    "                                drop_path_rate=0,\n",
    "                                ape=False,\n",
    "                                patch_norm=True,\n",
    "                                use_checkpoint=False).cuda()\n",
    "state_dict = torch.load(ckpt_path, map_location='cpu')\n",
    "G.load_state_dict(state_dict['G'])\n",
    "G.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf5e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_loader(fn):\n",
    "    data = np.load(fn)\n",
    "    n_channel = data.shape[0]\n",
    "    image = torch.from_numpy(data.astype(np.float32)).unsqueeze(0)\n",
    "    output = [image[:, i, :, :].unsqueeze(0).detach() for i in range(n_channel)]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e88424e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, inputs, targets):\n",
    "    contrast_input = inputs\n",
    "    contrast_output = targets\n",
    "    img_inputs = [data[i].detach().cuda() for i in contrast_input]\n",
    "    img_targets = [data[i].detach().cuda() for i in contrast_output]\n",
    "    return img_inputs, img_targets, contrast_input, contrast_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a75eb9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import interpolate\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "def sum_attn(attn_maps, head_normalization=True):\n",
    "    n_heads = [24, 24, 12, 12, 6, 6, 3, 3]\n",
    "    attn = torch.zeros_like(attn_maps[-1][:, :, :, :, 0])\n",
    "    for n_head, attn_map in zip(n_heads, attn_maps):\n",
    "        attn_map = attn_map[:, :, :, :, 0]\n",
    "        scale_factor = 48//attn_map.shape[-1]\n",
    "        attn_map = interpolate(attn_map, size=(40, 48))/scale_factor**2\n",
    "        if head_normalization:\n",
    "            attn_map = attn_map/n_head\n",
    "        attn += attn_map\n",
    "    return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a1d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = slice_loader('/mnt/raid/jiang/projects/SubtleGAN/data/brats2021_slices_crop192x160/test/BraTS2021_01409/077.npy')\n",
    "data = [d.detach().cuda() for d in data]\n",
    "img_inputs, img_targets, contrast_input, contrast_output = split_data(data, [0,2,3], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f44b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn output torch.Size([1, 6, 160, 192])\n"
     ]
    }
   ],
   "source": [
    "outputs, _, attn = G(img_inputs, contrast_input, contrast_output, return_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_sum = sum_attn(attn)\n",
    "att_sum = softmax(att_sum, dim = 1)\n",
    "att_sum = torch.rot90(att_sum, 3, [2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e5f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_sum = att_sum.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d4fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(att_sum[0, 0, :, :])\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "#plt.clim(0, 1)\n",
    "plt.savefig(\"Fig/attn_t1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(att_sum[0, 1, :, :])\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.savefig(\"Fig/attn_t2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ff48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(att_sum[0, 2, :, :])\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.savefig(\"Fig/attn_flair.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c520ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_last = torch.rot90(attn[-1], 3, [2, 3])\n",
    "att_last = softmax(att_last[:, :, :, :, 0], dim=1)\n",
    "att_last = att_last.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3556389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(att_last[0, 0, :, :])\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.savefig(\"Fig/attn_t1_last.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(att_last[0, 1, :, :])\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.savefig(\"Fig/attn_t2_last.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4adaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(att_last[0, 2, :, :])\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.savefig(\"Fig/attn_flair_last.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05871b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mmt)",
   "language": "python",
   "name": "mmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
